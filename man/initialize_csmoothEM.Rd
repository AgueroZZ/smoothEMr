% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/06_cSmoothEM.R
\name{initialize_csmoothEM}
\alias{initialize_csmoothEM}
\title{Initialize csmoothEM (coordinate-specific SmoothEM)}
\usage{
initialize_csmoothEM(
  X,
  method = c("tSNE", "PCA", "pcurve", "random", "fiedler", "multi_scale", "isomap"),
  rw_q = 2,
  lambda = 1,
  relative_lambda = TRUE,
  K = NULL,
  num_iter = 1,
  modelName = c("homoskedastic", "heteroskedastic"),
  ridge = 0,
  nugget = 0,
  eigen_tol = NULL,
  include.data = TRUE,
  adaptive = "ml",
  lambda_min = 1e-08,
  lambda_max = 1e+08,
  sigma_update = c("ml", "mstep"),
  sigma_min = 1e-10,
  sigma_max = 1e+10,
  discretization = c("equal", "quantile", "kmeans"),
  ...
)
}
\arguments{
\item{X}{n-by-d numeric matrix.}

\item{method}{One of \code{"tSNE","PCA","random","fiedler","multi_scale", "pcurve","isomap"}.
Currently \code{"multi_scale"} is not implemented for csmoothEM and will error.}

\item{rw_q}{Integer RW order along K for \code{Q_K}.}

\item{lambda}{Scalar or length-d vector. If scalar, recycled to length d.
Used as an initial value when \code{adaptive="none"}.}

\item{relative_lambda}{Logical; if TRUE, scale the base prior for coordinate \code{j} by a
coordinate-specific variance scale (see Details).}

\item{K}{Number of mixture components. If NULL, defaults to \code{min(50, floor(n/5))} (at least 2).}

\item{num_iter}{Integer >= 1; number of warm-start iterations to run immediately.}

\item{modelName}{Either \code{"homoskedastic"} or \code{"heteroskedastic"}.}

\item{ridge}{Nonnegative ridge added when building RW precision \code{Q_K}.}

\item{nugget}{Nonnegative nugget used in M-step variance updates.}

\item{eigen_tol}{Optional eigen tolerance used for generalized log-determinants (if needed downstream).}

\item{include.data}{Logical; store the (kept) data matrix in the returned object.}

\item{adaptive}{Character specifying how to initialize/update \code{lambda_vec}:
\describe{
  \item{\code{"none"}}{Do not estimate \code{lambda_vec} from the initialization; use \code{lambda}.}
  \item{\code{"prior"}}{Prior-based estimate using the initialized component means \eqn{\mu}.}
  \item{\code{"ml"}}{Collapsed-ML warm start (dispatches to \code{do_csmoothEM_ml_collapsed()} inside
    \code{do_csmoothEM()}), allowing optional ML updates of \eqn{\sigma^2}.}
}
Logical values are accepted for backward compatibility: \code{TRUE} is treated as \code{"prior"}
and \code{FALSE} as \code{"none"}.}

\item{lambda_min, lambda_max}{Positive bounds for \code{lambda_vec} (used when \code{adaptive!="none"}).}

\item{sigma_update}{Character. Only used when \code{adaptive="ml"} during the warm start.
Either \code{"mstep"} or \code{"ml"}; see \code{\link{do_csmoothEM_ml_collapsed}}.}

\item{sigma_min, sigma_max}{Positive bounds for \code{sigma2} when \code{adaptive="ml"} and
\code{sigma_update="ml"}.}

\item{discretization}{Discretization method passed to \code{initialize_ordering_csmooth()}.}

\item{...}{Passed to the ordering method (e.g. PCA/tSNE/pcurve/fiedler/isomap).}
}
\value{
A \code{csmooth_em} object.
}
\description{
Create a \code{csmooth_em} object using the same ordering-initialization ideas as
\code{initialize_smoothEM()}, but with coordinate-specific penalties \code{lambda_vec}
and diagonal covariance structures only.

Workflow:
\enumerate{
  \item Initialize an ordering and discretize into \code{K} components using
  \code{initialize_ordering_csmooth()}.
  \item Build the base random-walk precision \code{Q_K} along components (with \code{lambda=1}).
  \item Initialize \code{lambda_vec}. If \code{adaptive} is not \code{"none"}, estimate an initial
  \code{lambda_vec} from the initialized parameters (and clamp to \code{[lambda_min, lambda_max]}).
  \item Construct a \code{csmooth_em} object via \code{as_csmooth_em()}, then run a warm start via
  \code{do_csmoothEM()} for \code{num_iter} iterations.
}
}
\details{
Let \eqn{Q_K} denote the RW(q) precision along components. For a separable prior
\eqn{Q_{\mathrm{full}} = I_d \otimes Q_K}, the rank deficiency is \eqn{rw_q} per coordinate,
so a convenient degrees-of-freedom term is \eqn{r = K - rw_q}.

When \code{adaptive = "prior"}, the initializer estimates each coordinate penalty by:
\deqn{\lambda_j = r \big/ \left( \mu_{j\cdot}^\top Q_{j,\mathrm{base}} \mu_{j\cdot} \right),}
where \eqn{\mu_{j\cdot}} is the length-K vector of component means for coordinate \eqn{j}, and
\eqn{Q_{j,\mathrm{base}}} is the coordinate-specific base precision returned by
\code{.compute_Qbase_j()} (equivalently, \eqn{Q_K} possibly rescaled when
\code{relative_lambda = TRUE}). The estimate is clamped to \eqn{[\text{lambda_min},\text{lambda_max}]}.

When \code{adaptive = "ml"}, the warm start uses the collapsed-ML routine. In this mode,
\code{do_csmoothEM()} dispatches to \code{do_csmoothEM_ml_collapsed()}, which optimizes a
collapsed (Laplace-exact, Gaussian) objective and records \code{ml_trace} as the collapsed
objective \eqn{\mathcal{C}}.

When \code{relative_lambda = TRUE}, the base precision \eqn{Q_{j,\mathrm{base}}} is scaled by a
coordinate variance proxy:
\itemize{
  \item \code{modelName = "homoskedastic"}: scale by \code{1 / sigma2[j]}.
  \item \code{modelName = "heteroskedastic"}: scale by \code{1 / sum_k pi_k sigma2[j,k]}.
}
}
