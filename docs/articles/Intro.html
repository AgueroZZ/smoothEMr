<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Intro to smoothEMr • smoothEMr</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Intro to smoothEMr">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">smoothEMr</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles

    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Intro.html">Intro to smoothEMr</a>
    </li>
    <li>
      <a href="../articles/partition.html">Introduction to coordinate partitioning with smoothEMr</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Intro to smoothEMr</h1>
            
      

      <div class="hidden name"><code>Intro.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="model-overview">Model overview<a class="anchor" aria-label="anchor" href="#model-overview"></a>
</h2>
<p><code>smoothEMr</code> fits a Gaussian mixture model with a
structured (smoothness) prior on the component means. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">X_i \in \mathbb{R}^d</annotation></semantics></math>
be the observed vector for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">i = 1,\dots,n</annotation></semantics></math>,
and let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>K</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">z_i \in \{1,\dots,K\}</annotation></semantics></math>
be the latent component label. The observation model is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mi>k</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>μ</mi><mi>k</mi></msub><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mspace width="2.0em"></mspace><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>π</mi><mi>k</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
X_i \mid z_i = k \sim N(\mu_k, \Sigma),
\qquad \Pr(z_i = k) = \pi_k .
</annotation></semantics></math></p>
<p>To encourage <em>ordered / smooth transitions</em> across components
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">k=1,\dots,K</annotation></semantics></math>,
we place a multivariate normal prior on the stacked mean vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>μ</mi><mn>1</mn><mi>⊤</mi></msubsup><mo>,</mo><mi>…</mi><mo>,</mo><msubsup><mi>μ</mi><mi>K</mi><mi>⊤</mi></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">U = (\mu_1^\top, \dots, \mu_K^\top)^\top</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msubsup><mi>Q</mi><mi>λ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
U \sim N(0, Q_\lambda^{-1}),
</annotation></semantics></math></p>
<p>where the precision matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Q</mi><mi>λ</mi></msub><annotation encoding="application/x-tex">Q_\lambda</annotation></semantics></math>
encodes the desired structure across components. For computational
efficiency, we require
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Q</mi><mi>λ</mi></msub><annotation encoding="application/x-tex">Q_\lambda</annotation></semantics></math>
to be sparse so the prior is a GMRF (Gaussian Markov random field). Many
common GMRF priors can be interpreted as discrete analogues of
continuous GP priors. Equivalently, the M-step solves a penalized
optimization problem with quadratic penalty
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac displaystyle="false"><mn>1</mn><mn>2</mn></mfrac><msup><mi>U</mi><mi>⊤</mi></msup><msub><mi>Q</mi><mi>λ</mi></msub><mi>U</mi></mrow><annotation encoding="application/x-tex">\tfrac{1}{2} U^\top Q_\lambda U</annotation></semantics></math>.</p>
<p>The default choice in <code>smoothEMr</code> is a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math>-th
order random-walk (RW) prior, with</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>λ</mi></msub><mo>=</mo><mi>λ</mi><msub><mi>Q</mi><mrow><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">w</mi></mrow><mi>q</mi></mrow></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">
Q_\lambda = \lambda Q_{\mathrm{rw}q},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Q</mi><mrow><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">w</mi></mrow><mi>q</mi></mrow></msub><annotation encoding="application/x-tex">Q_{\mathrm{rw}q}</annotation></semantics></math>
is the precision matrix induced by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math>-th
order discrete differences. Larger
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
enforces stronger smoothness across neighboring components.</p>
<p>Equivalently, letting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>μ</mi><mrow><mi>⋅</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>μ</mi><mrow><mn>1</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>μ</mi><mrow><mi>K</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">\mu_{\cdot,j} = (\mu_{1,j},\dots,\mu_{K,j})^\top \in \mathbb{R}^K</annotation></semantics></math>,
the RW2 penalty can be written as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow><mi>K</mi></munderover><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><msub><mi>μ</mi><mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>−</mo><mn>2</mn><msub><mi>μ</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>μ</mi><mrow><mi>k</mi><mo>−</mo><mn>2</mn><mo>,</mo><mi>j</mi></mrow></msub><msup><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mn>2</mn></msup><mspace width="0.278em"></mspace><mo>=</mo><mspace width="0.278em"></mspace><msubsup><mi>μ</mi><mrow><mi>⋅</mi><mo>,</mo><mi>j</mi></mrow><mi>⊤</mi></msubsup><msub><mi>Q</mi><mrow><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">w</mi></mrow><mn>2</mn></mrow></msub><mspace width="0.167em"></mspace><msub><mi>μ</mi><mrow><mi>⋅</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mspace width="2.0em"></mspace><msub><mi>Q</mi><mrow><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">w</mi></mrow><mn>2</mn></mrow></msub><mo>=</mo><msubsup><mi>D</mi><mn>2</mn><mi>⊤</mi></msubsup><msub><mi>D</mi><mn>2</mn></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">
\sum_{k=3}^K \big(\mu_{k,j} - 2\mu_{k-1,j} + \mu_{k-2,j}\big)^2
\;=\;
\mu_{\cdot,j}^\top Q_{\mathrm{rw}2}\,\mu_{\cdot,j},
\qquad
Q_{\mathrm{rw}2} = D_2^\top D_2,
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mn>2</mn></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>K</mi><mo>−</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">D_2 \in \mathbb{R}^{(K-2)\times K}</annotation></semantics></math>
is the second-difference matrix with rows
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>…</mi><mo>,</mo><mn>0</mn><mo>,</mo><mspace width="0.167em"></mspace><mn>1</mn><mo>,</mo><mspace width="0.167em"></mspace><mo>−</mo><mn>2</mn><mo>,</mo><mspace width="0.167em"></mspace><mn>1</mn><mo>,</mo><mspace width="0.167em"></mspace><mn>0</mn><mo>,</mo><mi>…</mi><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,\dots,0,\,1,\,-2,\,1,\,0,\dots,0)</annotation></semantics></math>.</p>
<p>By default, <code>smoothEMr</code> assumes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
is diagonal and shared across components (homoskedasticity), but the
framework can be extended to accommodate heteroskedasticity or other
covariance structures.</p>
</div>
<div class="section level2">
<h2 id="algorithm-overview-smooth-em">Algorithm overview (smooth-EM)<a class="anchor" aria-label="anchor" href="#algorithm-overview-smooth-em"></a>
</h2>
<p>Fix a smoothing strength
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
and a sparse precision matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>λ</mi></msub><mo>=</mo><mi>λ</mi><msub><mi>Q</mi><mrow><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">w</mi></mrow><mi>q</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_\lambda = \lambda Q_{\mathrm{rw}q}</annotation></semantics></math>.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>μ</mi><mn>1</mn><mi>⊤</mi></msubsup><mo>,</mo><mi>…</mi><mo>,</mo><msubsup><mi>μ</mi><mi>K</mi><mi>⊤</mi></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">U = (\mu_1^\top,\dots,\mu_K^\top)^\top</annotation></semantics></math>
stack all component means. We seek a MAP estimate by maximizing the
log-posterior (up to an additive constant) over
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mo>,</mo><mi>U</mi><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\pi,U,\Sigma)</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mo>,</mo><mi>U</mi><mo>,</mo><mi>Σ</mi><mo>;</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo>log</mo><mspace width="-0.167em"></mspace><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">(</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>π</mi><mi>k</mi></msub><mspace width="0.167em"></mspace><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>μ</mi><mi>k</mi></msub><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo><mspace width="0.278em"></mspace><mo>−</mo><mspace width="0.278em"></mspace><mfrac displaystyle="false"><mn>1</mn><mn>2</mn></mfrac><msup><mi>U</mi><mi>⊤</mi></msup><msub><mi>Q</mi><mi>λ</mi></msub><mi>U</mi><mspace width="0.278em"></mspace><mo>+</mo><mspace width="0.278em"></mspace><mtext mathvariant="normal">const</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
J(\pi,U,\Sigma;\lambda)
=
\sum_{i=1}^n \log\!\Big(\sum_{k=1}^K \pi_k\, p(X_i \mid \mu_k,\Sigma)\Big)
\;-\; \tfrac{1}{2} U^\top Q_\lambda U
\;+\; \text{const}(\lambda),
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">const</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{const}(\lambda)</annotation></semantics></math>
collects terms that do not depend on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mo>,</mo><mi>U</mi><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\pi,U,\Sigma)</annotation></semantics></math>.</p>
<p>The smooth-EM algorithm performs coordinate ascent on an ELBO
(evidence lower bound)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">E</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">B</mi><mi mathvariant="normal">O</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mo>,</mo><mi>π</mi><mo>,</mo><mi>U</mi><mo>,</mo><mi>Σ</mi><mo>;</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{ELBO}(\Gamma,\pi,U,\Sigma;\lambda)</annotation></semantics></math>
of the form
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">E</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">B</mi><mi mathvariant="normal">O</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mo>,</mo><mi>π</mi><mo>,</mo><mi>U</mi><mo>,</mo><mi>Σ</mi><mo>;</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>γ</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">{</mo><mo>log</mo><msub><mi>π</mi><mi>k</mi></msub><mo>+</mo><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>μ</mi><mi>k</mi></msub><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">}</mo><mspace width="0.278em"></mspace><mo>+</mo><mspace width="0.278em"></mspace><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.278em"></mspace><mo>−</mo><mspace width="0.278em"></mspace><mfrac displaystyle="false"><mn>1</mn><mn>2</mn></mfrac><msup><mi>U</mi><mi>⊤</mi></msup><msub><mi>Q</mi><mi>λ</mi></msub><mi>U</mi><mspace width="0.278em"></mspace><mo>+</mo><mspace width="0.278em"></mspace><mtext mathvariant="normal">const</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
\mathrm{ELBO}(\Gamma,\pi,U,\Sigma;\lambda)
=
\sum_{i=1}^n \sum_{k=1}^K \gamma_{ik}
\Big\{
\log \pi_k + \log p(X_i \mid \mu_k,\Sigma)
\Big\}
\;+\; H(\Gamma)
\;-\; \tfrac{1}{2} U^\top Q_\lambda U
\;+\; \text{const}(\lambda),
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><msub><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><msub><mi>γ</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>log</mo><msub><mi>γ</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">H(\Gamma) = -\sum_{i,k}\gamma_{ik}\log\gamma_{ik}</annotation></semantics></math>
is the entropy term.</p>
<p><strong>Relationship between ELBO and the objective.</strong> For any
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mo>,</mo><mi>U</mi><mo>,</mo><mi>Σ</mi><mo>;</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.278em"></mspace><mo>≥</mo><mspace width="0.278em"></mspace><mrow><mi mathvariant="normal">E</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">B</mi><mi mathvariant="normal">O</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mo>,</mo><mi>π</mi><mo>,</mo><mi>U</mi><mo>,</mo><mi>Σ</mi><mo>;</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
J(\pi,U,\Sigma;\lambda) \;\ge\; \mathrm{ELBO}(\Gamma,\pi,U,\Sigma;\lambda),
</annotation></semantics></math> and equality holds when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>γ</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>=</mo><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mi>k</mi><mo>∣</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><mi>π</mi><mo>,</mo><mi>U</mi><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma_{ik} = \Pr(z_i=k \mid X_i,\pi,U,\Sigma)</annotation></semantics></math>
(i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>
equals the posterior responsibilities). Thus, maximizing the ELBO
provides a monotone lower-bound ascent procedure for the MAP objective
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>.</p>
<p>Given current parameter values, each iteration alternates:</p>
<ol style="list-style-type: decimal">
<li><p><strong>E-step (update
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>):</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>γ</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>∝</mo><msub><mi>π</mi><mi>k</mi></msub><mspace width="0.167em"></mspace><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>μ</mi><mi>k</mi></msub><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mspace width="1.0em"></mspace><mrow><mtext mathvariant="normal">normalized over </mtext><mspace width="0.333em"></mspace></mrow><mi>k</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>K</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
\gamma_{ik} \propto \pi_k\, p(X_i \mid \mu_k,\Sigma),
\quad \text{normalized over } k=1,\dots,K.
</annotation></semantics></math></p></li>
<li><p><strong>M-step (update
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo>,</mo><mi>U</mi><mo>,</mo><mi>Σ</mi></mrow><annotation encoding="application/x-tex">\pi,U,\Sigma</annotation></semantics></math>):</strong>
update
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
in closed form given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>,
and update
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>
by maximizing
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>←</mo><mo>arg</mo><munder><mo>max</mo><mi>U</mi></munder><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">{</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></munder><msub><mi>γ</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>μ</mi><mi>k</mi></msub><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.278em"></mspace><mo>−</mo><mspace width="0.278em"></mspace><mfrac displaystyle="false"><mn>1</mn><mn>2</mn></mfrac><msup><mi>U</mi><mi>⊤</mi></msup><msub><mi>Q</mi><mi>λ</mi></msub><mi>U</mi><mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">}</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
U \leftarrow \arg\max_U
\Big\{
\sum_{i,k} \gamma_{ik}\log p(X_i \mid \mu_k,\Sigma)
\;-\; \tfrac{1}{2} U^\top Q_\lambda U
\Big\}.
</annotation></semantics></math></p></li>
</ol>
<p>With
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
fixed, the ELBO is non-decreasing over iterations; moreover, because
each E-step makes the bound tight at the current parameters, the MAP
objective
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>
is also non-decreasing.</p>
</div>
<div class="section level2">
<h2 id="a-simple-2d-example">A simple 2D example<a class="anchor" aria-label="anchor" href="#a-simple-2d-example"></a>
</h2>
<p>We simulate data from a spiral (swiss-roll) embedded in 2D space,
then fit a smoothEM model.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sim</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/simulate_swiss_roll_1d_2d.html">simulate_swiss_roll_1d_2d</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1500</span>, sigma <span class="op">=</span> <span class="fl">0.2</span>, seed <span class="op">=</span> <span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">sim</span><span class="op">$</span><span class="va">obs</span><span class="op">$</span><span class="va">x</span>, <span class="va">sim</span><span class="op">$</span><span class="va">obs</span><span class="op">$</span><span class="va">y</span>, pch <span class="op">=</span> <span class="fl">16</span>, cex <span class="op">=</span> <span class="fl">0.35</span>, col <span class="op">=</span> <span class="st">"grey80"</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"x"</span>, ylab <span class="op">=</span> <span class="st">"y"</span>, main <span class="op">=</span> <span class="st">"Noisy 2D swiss-roll (spiral)"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pal</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/grDevices/colorRamp.html" class="external-link">colorRampPalette</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"navy"</span>, <span class="st">"cyan"</span>, <span class="st">"yellow"</span>, <span class="st">"red"</span><span class="op">)</span><span class="op">)</span><span class="op">(</span><span class="fl">256</span><span class="op">)</span></span>
<span><span class="va">t_scaled</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">sim</span><span class="op">$</span><span class="va">t</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">sim</span><span class="op">$</span><span class="va">t</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">sim</span><span class="op">$</span><span class="va">t</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">sim</span><span class="op">$</span><span class="va">t</span><span class="op">)</span><span class="op">)</span>  <span class="co"># in [0,1]</span></span>
<span><span class="va">col_t</span> <span class="op">&lt;-</span> <span class="va">pal</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">pmax</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">pmin</a></span><span class="op">(</span><span class="fl">256</span>, <span class="fl">1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">floor</a></span><span class="op">(</span><span class="fl">255</span> <span class="op">*</span> <span class="va">t_scaled</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html" class="external-link">points</a></span><span class="op">(</span><span class="va">sim</span><span class="op">$</span><span class="va">truth</span><span class="op">$</span><span class="va">x</span>, <span class="va">sim</span><span class="op">$</span><span class="va">truth</span><span class="op">$</span><span class="va">y</span>, pch <span class="op">=</span> <span class="fl">16</span>, cex <span class="op">=</span> <span class="fl">0.25</span>, col <span class="op">=</span> <span class="va">col_t</span><span class="op">)</span></span></code></pre></div>
<p><img src="Intro_files/figure-html/unnamed-chunk-2-1.png" class="r-plt" width="576"></p>
<p>To fit the model, we first initialize the smoothEM algorithm using a
given initialization method. Here, we use <code>fiedler</code>
initialization and fix the smoothing parameter <code>lambda</code> to
10.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/initialize_csmoothEM.html">initialize_csmoothEM</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">sim</span><span class="op">$</span><span class="va">obs</span><span class="op">)</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"fiedler"</span>,</span>
<span>  adaptive <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  lambda <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="op">)</span></span>
<span><span class="va">fit</span></span>
<span><span class="co">#&gt; Fitted csmoothEM object with RW(2) prior along K = 50</span></span>
<span><span class="co">#&gt; -----</span></span>
<span><span class="co">#&gt;   n = 1500, d = 2, modelName = homoskedastic</span></span>
<span><span class="co">#&gt;   iter = 1; init_method = fiedler; adaptive = none;</span></span>
<span><span class="co">#&gt;   lambda_vec: range=[10, 10], mean=10, relative = TRUE</span></span>
<span><span class="co">#&gt;   ELBO last = -10676.855810; penLogLik last = -10628.574088; ML/C last = -10629.182592</span></span></code></pre></div>
<p>The key function <code><a href="../reference/do_csmoothEM.html">do_csmoothEM()</a></code> runs smooth-EM updates
for a given number of iterations.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="va">fit</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/do_csmoothEM.html">do_csmoothEM</a></span><span class="op">(</span>iter <span class="op">=</span> <span class="fl">30</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span></code></pre></div>
<p><img src="Intro_files/figure-html/unnamed-chunk-4-1.png" class="r-plt" width="576"></p>
<p>We can also inspect ELBO values over iterations:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit</span>, plot_type <span class="op">=</span> <span class="st">"elbo"</span><span class="op">)</span></span></code></pre></div>
<p><img src="Intro_files/figure-html/unnamed-chunk-5-1.png" class="r-plt" width="576"></p>
</div>
<div class="section level2">
<h2 id="adaptive-estimation-of-the-smoothing-parameter-lambda">Adaptive estimation of the smoothing parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#adaptive-estimation-of-the-smoothing-parameter-lambda"></a>
</h2>
<p>Optionally, smoothEMr can update
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
during the EM iterations. Unlike the MAP updates performed when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
is fixed, here we aim to optimize the marginal likelihood
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>∣</mo><mi>η</mi><mo>,</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>log</mo><mo>∫</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>∣</mo><mi>μ</mi><mo>,</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>∣</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>μ</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
\ell(\lambda)=\log p(X\mid \eta,\lambda)=\log\int p(X\mid \mu,\eta)\,p(\mu\mid \lambda)\,d\mu .
</annotation></semantics></math></p>
<p>The marginal likelihood is generally intractable. Using the ELBO as a
surrogate for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>∣</mo><mi>μ</mi><mo>,</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log p(X\mid \mu,\eta)</annotation></semantics></math>
yields an approximate marginal likelihood
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mo>ℓ</mo><mo accent="true">̃</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>log</mo><mo>∫</mo><mo>exp</mo><mspace width="-0.167em"></mspace><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mrow><mi mathvariant="normal">E</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">B</mi><mi mathvariant="normal">O</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>q</mi><mo>;</mo><mi>μ</mi><mo>,</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>∣</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>μ</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
\widetilde{\ell}(\lambda)
=\log\int \exp\!\big(\mathrm{ELBO}(q;\mu,\eta)\big)\,p(\mu\mid \lambda)\,d\mu .
</annotation></semantics></math></p>
<p>Because the ELBO is (locally) quadratic in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
and the prior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>∣</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\mu\mid \lambda)</annotation></semantics></math>
is Gaussian, we can further obtain a closed-form Laplace approximation.
Let
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo>=</mo><mo>arg</mo><munder><mo>max</mo><mi>μ</mi></munder><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">{</mo><mrow><mi mathvariant="normal">E</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">B</mi><mi mathvariant="normal">O</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>q</mi><mo>;</mo><mi>μ</mi><mo>,</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>∣</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">}</mo><mo>,</mo><mspace width="2.0em"></mspace><mi>H</mi><mo>=</mo><mi>−</mi><msubsup><mi>∇</mi><mi>μ</mi><mn>2</mn></msubsup><mspace width="-0.167em"></mspace><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">[</mo><mrow><mi mathvariant="normal">E</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">B</mi><mi mathvariant="normal">O</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>q</mi><mo>;</mo><mi>μ</mi><mo>,</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>∣</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">]</mo><msub><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">|</mo><mrow><mi>μ</mi><mo>=</mo><mover><mi>μ</mi><mo accent="true">̂</mo></mover></mrow></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
\widehat{\mu}
=\arg\max_{\mu}\Big\{\mathrm{ELBO}(q;\mu,\eta)+\log p(\mu\mid \lambda)\Big\},
\qquad
H=-\nabla_\mu^2\!\Big[\mathrm{ELBO}(q;\mu,\eta)+\log p(\mu\mid \lambda)\Big]\Big|_{\mu=\widehat{\mu}} .
</annotation></semantics></math> Then the collapsed objective is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>q</mi><mo>;</mo><mi>η</mi><mo>,</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mi mathvariant="normal">E</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">B</mi><mi mathvariant="normal">O</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>q</mi><mo>;</mo><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo>∣</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>K</mi><mn>2</mn></mfrac><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>log</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>H</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
C(q;\eta,\lambda)
=\mathrm{ELBO}(q;\widehat{\mu},\eta)+\log p(\widehat{\mu}\mid \lambda)
+\frac{K}{2}\log(2\pi)-\frac{1}{2}\log|H|.
</annotation></semantics></math></p>
<p>To implement adaptive estimation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>,
we just need to use <code>adaptive = "ml"</code> in the initialization
step.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">fit_ml</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/initialize_csmoothEM.html">initialize_csmoothEM</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">sim</span><span class="op">$</span><span class="va">obs</span><span class="op">)</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"fiedler"</span>,</span>
<span>  adaptive <span class="op">=</span> <span class="st">"ml"</span></span>
<span><span class="op">)</span></span>
<span><span class="va">fit_ml</span></span>
<span><span class="co">#&gt; Fitted csmoothEM object with RW(2) prior along K = 50</span></span>
<span><span class="co">#&gt; -----</span></span>
<span><span class="co">#&gt;   n = 1500, d = 2, modelName = homoskedastic</span></span>
<span><span class="co">#&gt;   iter = 1; init_method = fiedler; adaptive = ml;</span></span>
<span><span class="co">#&gt;   lambda_vec: range=[9.5, 14.1], mean=11.8, relative = TRUE</span></span>
<span><span class="co">#&gt;   ELBO last = -10672.494762; penLogLik last = -10623.434999; ML/C last = -10628.660896</span></span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_ml</span> <span class="op">&lt;-</span> <span class="va">fit_ml</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/do_csmoothEM.html">do_csmoothEM</a></span><span class="op">(</span>iter <span class="op">=</span> <span class="fl">30</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_ml</span><span class="op">)</span></span></code></pre></div>
<p><img src="Intro_files/figure-html/unnamed-chunk-7-1.png" class="r-plt" width="576"></p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_ml</span>, plot_type <span class="op">=</span> <span class="st">"elbo"</span><span class="op">)</span></span></code></pre></div>
<p><img src="Intro_files/figure-html/unnamed-chunk-8-1.png" class="r-plt" width="576"></p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Ziang Zhang.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

      </footer>
</div>






  </body>
</html>
